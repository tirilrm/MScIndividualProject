{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_functions' from '/Users/tiril/Documents/IndividualProject/nuclear_repo/knowledge_graphs/helper_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helper_functions as hf\n",
    "importlib.reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC is present in 33 articles (0.92%), Highest frequency: 23, Total frequency: 213 (in https://www.neimagazine.com/news/arc-clean-technology-signs-agreement-on-deployment-of-smrs-in-alberta-10708319/)\n",
      "Babcock and Wilcox is present in 13 articles (0.36%), Highest frequency: 4, Total frequency: 20 (in https://www.newcivilengineer.com/latest/pm-announces-national-endeavour-to-build-nuclear-workforce-and-warns-of-china-threat-26-03-2024/)\n",
      "Berkeley is present in 21 articles (0.58%), Highest frequency: 9, Total frequency: 47 (in https://www.neimagazine.com/news/first-concrete-box-of-radioactive-waste-safely-transferred-from-berkeley-npp-10531964/)\n",
      "BWX is present in 54 articles (1.50%), Highest frequency: 19, Total frequency: 191 (in https://www.neimagazine.com/news/bwxt-to-evaluate-microreactor-deployment-in-wyoming-11146865/)\n",
      "Elysium is present in 1 articles (0.03%), Highest frequency: 1, Total frequency: 1 (in https://www.neimagazine.com/news/more-funding-for-us-companies-under-does-gain-programme-10451593/)\n",
      "Flibe is present in 11 articles (0.31%), Highest frequency: 9, Total frequency: 32 (in https://www.neimagazine.com/news/kairos-claims-operation-of-worlds-largest-flibe-system-11315280/)\n",
      "Framatome is present in 165 articles (4.59%), Highest frequency: 18, Total frequency: 521 (in https://www.neimagazine.com/news/framatome-expands-nuclear-co-operation-with-hungary-11150465/)\n",
      "GE Hitachi is present in 219 articles (6.09%), Highest frequency: 17, Total frequency: 626 (in https://www.neimagazine.com/news/uk-funding-for-bwrx-300-11473443/)\n",
      "General Atomics is present in 29 articles (0.81%), Highest frequency: 10, Total frequency: 63 (in https://www.neimagazine.com/news/general-atomics-announces-plans-for-fusion-pilot-plant-10122682/)\n",
      "HolosGen is present in 1 articles (0.03%), Highest frequency: 3, Total frequency: 3 (in https://www.neimagazine.com/news/inl-sees-market-for-microreactors-in-some-us-states-10884680/)\n",
      "Holtec International is present in 111 articles (3.09%), Highest frequency: 18, Total frequency: 644 (in https://www.world-nuclear-news.org/Articles/Accord-sees-mass-deployment-of-Holtec-SMRs-in-Ukra)\n",
      "Hyperion Power is present in 0 articles (0.00%), Highest frequency: 0, Total frequency: 0 (in None)\n",
      "Kairos Power is present in 18 articles (0.50%), Highest frequency: 18, Total frequency: 115 (in https://www.neimagazine.com/news/kairos-power-to-produce-fuel-for-hermes-demonstration-reactor-10433303/)\n",
      "Moltex Energy is present in 30 articles (0.83%), Highest frequency: 16, Total frequency: 172 (in https://www.neimagazine.com/analysis/the-next-step-in-molten-salt-reactor-evolution-10902253/)\n",
      "NANO Nuclear is present in 7 articles (0.19%), Highest frequency: 13, Total frequency: 44 (in https://www.neimagazine.com/news/nano-plans-fuel-fabrication-plant-at-idaho-national-laboratory-11096442/)\n",
      "NuScale is present in 197 articles (5.48%), Highest frequency: 27, Total frequency: 1090 (in https://www.nucnet.org/news/small-modular-reactor-company-says-misleading-research-report-has-no-basis-in-fact-11-1-2023)\n",
      "Oak Ridge National Laboratory is present in 68 articles (1.89%), Highest frequency: 9, Total frequency: 170 (in https://www.neimagazine.com/news/fusion-reactor-prototype-planned-for-tva-site-11552220/)\n",
      "Oklo is present in 27 articles (0.75%), Highest frequency: 24, Total frequency: 216 (in https://www.neimagazine.com/news/oklo-announces-merger-with-acquisition-company-altc-11005073/)\n",
      "StarCore Nuclear is present in 0 articles (0.00%), Highest frequency: 0, Total frequency: 0 (in None)\n",
      "TerraPower is present in 82 articles (2.28%), Highest frequency: 22, Total frequency: 354 (in https://www.neimagazine.com/news/terrapower-breaks-ground-on-planned-natrium-npp/)\n",
      "Terrestial is present in 0 articles (0.00%), Highest frequency: 0, Total frequency: 0 (in None)\n",
      "ThorCon is present in 9 articles (0.25%), Highest frequency: 32, Total frequency: 66 (in https://www.neimagazine.com/news/molten-salt-nuclear-power-barge-proposed-for-indonesia-10446084/)\n",
      "Ultra Safe Nuclear Corporation is present in 47 articles (1.31%), Highest frequency: 21, Total frequency: 277 (in https://www.neimagazine.com/news/u-battery-personnel-join-ultra-safe-nuclears-uk-team-10713528/)\n",
      "Westinghouse is present in 424 articles (11.79%), Highest frequency: 34, Total frequency: 1730 (in https://www.neimagazine.com/news/cameco-and-brookfield-renewable-to-acquire-westinghouse-10084610/)\n",
      "X-Energy is present in 88 articles (2.45%), Highest frequency: 17, Total frequency: 505 (in https://www.neimagazine.com/news/business-complications-for-smr-companies-x-energy-and-nuscale-11268599/)\n"
     ]
    }
   ],
   "source": [
    "# Get statistical properties of articles\n",
    "name_dict = {\n",
    "    'ARC': ['ARC'],\n",
    "    'Babcock and Wilcox': ['Babcock'],\n",
    "    'Berkeley': ['Berkeley'],\n",
    "    'BWX': ['BWX'],\n",
    "    'Elysium': ['Elysium'],\n",
    "    'Flibe': ['Flibe'],\n",
    "    'Framatome': ['Framatome'],\n",
    "    'GE Hitachi': ['Hitachi', 'GEH'],\n",
    "    'General Atomics': ['General Atomics'],\n",
    "    'HolosGen': ['HolosGen', 'Holos'],\n",
    "    'Holtec International': ['Holtec'],\n",
    "    'Hyperion Power': ['Hyperion'],\n",
    "    'Kairos Power': ['Kairos'],\n",
    "    'Moltex Energy': ['Moltex'],\n",
    "    'NANO Nuclear': ['NANO', 'NNE'],\n",
    "    'NuScale': ['NuScale'],\n",
    "    'Oak Ridge National Laboratory': ['Oak Ridge National Laboratory', 'ORNL'],\n",
    "    'Oklo': ['Oklo'],\n",
    "    'StarCore Nuclear': ['StarCore'],\n",
    "    'TerraPower': ['TerraPower'],\n",
    "    'Terrestial': ['Terrestial'],\n",
    "    'ThorCon': ['ThorCon'],\n",
    "    'Ultra Safe Nuclear Corporation': ['Ultra Safe Nuclear Corporation', 'USNC'],\n",
    "    'Westinghouse': ['Westinghouse', 'WEC'],\n",
    "    'X-Energy': ['X-energy']\n",
    "}\n",
    "\n",
    "filepath = 'data/articles_with_frequency.json'\n",
    "with open(filepath, 'r') as file:\n",
    "    articles = pd.read_json(filepath)\n",
    "\n",
    "base_directory = 'data/unfiltered'\n",
    "\n",
    "def get_articles(name, articles, base_directory, save=False):\n",
    "\n",
    "    subset = []\n",
    "    max_freq = 0\n",
    "    tot_freq = 0\n",
    "    url = None\n",
    "\n",
    "    for i in range(len(articles)):\n",
    "        article = articles.iloc[i]\n",
    "        freq = int(article[name])\n",
    "        tot_freq += freq\n",
    "        if freq > max_freq:\n",
    "            max_freq = freq\n",
    "            url = articles.iloc[i]['url']\n",
    "        if freq > 0:\n",
    "            subset.append({\n",
    "                'url': article['url'],\n",
    "                'title': article['title'],\n",
    "                'text': article['text'],\n",
    "                'frequency': freq \n",
    "            })\n",
    "\n",
    "    subset_length = len(subset)\n",
    "\n",
    "    if save:\n",
    "        filepath = f\"{base_directory}/{name.lower().replace(' ', '_')}/{name.lower().replace(' ', '_')}.json\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, 'w') as file:\n",
    "            json.dump(subset, file, indent=4, ensure_ascii=False)    \n",
    "    \n",
    "    print(f'{name} is present in {subset_length} articles ({(subset_length/len(articles))*100:.2f}%), Highest frequency: {max_freq}, Total frequency: {tot_freq} (in {url})')\n",
    "\n",
    "# Get articles loop\n",
    "for key, _ in name_dict.items():\n",
    "    get_articles(key, articles, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get basic statistics of the KGs\n",
    "def get_basic_stats(df):\n",
    "    head_counts = df['h_ent'].value_counts()\n",
    "    tail_counts = df['t_ent'].value_counts()\n",
    "    relation_counts = df['relation'].value_counts()\n",
    "\n",
    "    stats = {\n",
    "        'head_counts': head_counts.to_dict(),\n",
    "        'tail_counts': tail_counts.to_dict(),\n",
    "        'relations': relation_counts.to_dict()\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def print_stats(stats):\n",
    "    print(\"Entity Counts:\")\n",
    "    print(\"{:<15} | {:>10} | {:>10} | {:>10} | {:>10}\".format(\"Entity\", \"Head\", \"Tail\", \"Total\", \"Percent\"))\n",
    "    print(\"-\" * 67)\n",
    "    total_head = 0\n",
    "    total_tail = 0\n",
    "    entity_totals = []\n",
    "    for entity in set(stats['head_counts'].keys()).union(stats['tail_counts'].keys()):\n",
    "        head_count = stats['head_counts'].get(entity, 0)\n",
    "        tail_count = stats['tail_counts'].get(entity, 0)\n",
    "        entity_total = head_count + tail_count\n",
    "        entity_totals.append((entity, head_count, tail_count, entity_total))\n",
    "        total_head += head_count\n",
    "        total_tail += tail_count\n",
    "    grand_total = total_head + total_tail\n",
    "    entity_totals.sort(key=lambda x: x[3], reverse=True)\n",
    "    for entity, head, tail, total in entity_totals:\n",
    "        percent = (total / grand_total) * 100\n",
    "        print(\"{:<15} | {:>10} | {:>10} | {:>10} | {:>9.2f}%\".format(entity, head, tail, total, percent))\n",
    "    print(\"-\" * 67)\n",
    "    print(\"{:<15} | {:>10} | {:>10} | {:>10} | {:>9.2f}%\".format(\"Total\", total_head, total_tail, grand_total, 100.00))\n",
    "    print(\"\\nRelation Counts:\")\n",
    "    print(\"{:<20} | {:>10}\".format(\"Relation\", \"Count\"))\n",
    "    print(\"-\" * 32)\n",
    "    total_relations = sum(stats['relations'].values())\n",
    "    for relation, count in stats['relations'].items():\n",
    "        print(\"{:<20} | {:>10}\".format(relation, count))\n",
    "    print(\"-\" * 32)\n",
    "    print(\"{:<20} | {:>10}\".format(\"Total\", total_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Counts:\n",
      "Entity          |       Head |       Tail |      Total |    Percent\n",
      "-------------------------------------------------------------------\n",
      "LOC             |       4169 |       4020 |       8189 |     46.30%\n",
      "ORG             |       1259 |       3911 |       5170 |     29.23%\n",
      "PER             |       3053 |        441 |       3494 |     19.75%\n",
      "MISC            |        362 |        470 |        832 |      4.70%\n",
      "REACTOR         |          1 |          0 |          1 |      0.01%\n",
      "FUEL            |          0 |          1 |          1 |      0.01%\n",
      "SMR             |          0 |          1 |          1 |      0.01%\n",
      "-------------------------------------------------------------------\n",
      "Total           |       8844 |       8844 |      17688 |    100.00%\n",
      "\n",
      "Relation Counts:\n",
      "Relation             |      Count\n",
      "--------------------------------\n",
      "contains             |       4212\n",
      "company              |       2183\n",
      "nationality          |        787\n",
      "company location     |        719\n",
      "residence            |        182\n",
      "division             |        173\n",
      "contains division    |        163\n",
      "place of birth       |        104\n",
      "country of origin    |        104\n",
      "capital              |         62\n",
      "ethnicity            |         53\n",
      "ethnic background    |         41\n",
      "founded by           |         40\n",
      "child                |          7\n",
      "place of death       |          5\n",
      "industry             |          5\n",
      "religion             |          2\n",
      "neighborhood         |          1\n",
      "shareholder of       |          1\n",
      "--------------------------------\n",
      "Total                |       8844\n"
     ]
    }
   ],
   "source": [
    "# Final triplets\n",
    "data = pd.read_excel('data/final_triplets.xlsx')\n",
    "\n",
    "# All (i.e. raw) triplets\n",
    "if False:\n",
    "    with open('data/unfiltered/all_triplets.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        transformed_data = [{\n",
    "            'head': item['head']['word'],\n",
    "            'h_ent': item['head']['entity'],\n",
    "            'relation': item['relation'],\n",
    "            'tail': item['tail']['word'],\n",
    "            't_ent': item['tail']['entity']\n",
    "        } for item in data]\n",
    "    data = pd.DataFrame(transformed_data)\n",
    "\n",
    "# Get final statistical properties\n",
    "stats = get_basic_stats(data)\n",
    "print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Elysium: Source ('Elysium', 'ORG') is not in G\n",
      "Error processing HolosGen: Source ('HolosGen', 'ORG') is not in G\n",
      "Error processing Hyperion Power: Source ('Hyperion Power', 'ORG') is not in G\n",
      "Error processing StarCore Nuclear: Source ('StarCore Nuclear', 'ORG') is not in G\n",
      "Error processing Terrestial: Source ('Terrestial', 'ORG') is not in G\n"
     ]
    }
   ],
   "source": [
    "# Get basic statistics per KG at different prune levels\n",
    "with open('data/triplets_no_cutoff/graphs.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "def count_nodes(G):\n",
    "    counts = {\n",
    "        'PER': 0,\n",
    "        'ORG': 0,\n",
    "        'LOC': 0,\n",
    "        'MISC': 0,\n",
    "        'Unknown': 0,\n",
    "        'Other': 0,\n",
    "        'Total': 0\n",
    "    }\n",
    "\n",
    "    for _, attrs in G.nodes(data=True):\n",
    "        node_type = attrs.get('type', 'Other') \n",
    "        if node_type in counts:\n",
    "            counts[node_type] += 1\n",
    "            counts['Total'] += 1\n",
    "        else:\n",
    "            #print(node_type)\n",
    "            counts['Total'] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "statistics = {}\n",
    "depths = [1, 2, 3, 4, 5]\n",
    "for root in results:\n",
    "    root_node = (root, 'ORG')\n",
    "    try:\n",
    "        G = hf.make_graph(results[root])\n",
    "        statistics[root] = {}\n",
    "        statistics[root]['all'] = count_nodes(G)\n",
    "\n",
    "        for depth in depths:\n",
    "            G_copy = G.copy()\n",
    "            pruned_G = hf.prune_graph_by_depth(G_copy, root_node, depth, 'bidirectional')\n",
    "            pruned_count = count_nodes(pruned_G)\n",
    "            statistics[root][f'depth={depth}'] = pruned_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {root}: {e}\")\n",
    "        continue\n",
    "\n",
    "with open('results/basic_stats.json', 'w') as file:\n",
    "    json.dump(statistics, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def plot_selected_histograms(statistics, depth, show_percent=False, show_other=False):\n",
    "    skip_companies = ['Elysium', 'HolosGen', 'Hyperion Power', 'StarCore Nuclear', 'Terrestial']    \n",
    "    entity_colors = {\n",
    "        'LOC': '#ffb347',\n",
    "        'ORG': '#87ceeb',\n",
    "        'PER': '#90ee90',\n",
    "        'MISC': '#dda0dd',\n",
    "        'Other': '#d3d3d3'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 4, figsize=(12, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # First pass to determine the global maximum for common y-axis\n",
    "    max_frequency = 0\n",
    "    for company, data in statistics.items():\n",
    "        if company in skip_companies:\n",
    "            continue\n",
    "        \n",
    "        counts = data[depth]\n",
    "        known_tags_sum = sum(counts.get(tag, 0) for tag in ['PER', 'ORG', 'LOC', 'MISC'])\n",
    "        total_entities = sum(counts.values())\n",
    "        other_entities = total_entities - known_tags_sum\n",
    "\n",
    "        for tag in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "            max_frequency = max(max_frequency, counts.get(tag, 0)) + 0.2 # Increase to decrease max height of bars\n",
    "\n",
    "    index = 0\n",
    "    for company, data in statistics.items():\n",
    "        if company in skip_companies:\n",
    "            continue\n",
    "        if index >= 20:\n",
    "            break\n",
    "\n",
    "        counts = data[depth]\n",
    "        entity_tags = ['PER', 'ORG', 'LOC', 'MISC']\n",
    "        frequencies = [counts.get(tag, 0) for tag in entity_tags]\n",
    "        total_entities = sum(frequencies)  # Calculate total number of entities for percentages\n",
    "\n",
    "        # Calculate \"Other\" entities\n",
    "        if show_other:\n",
    "            known_tags_sum = sum(frequencies)\n",
    "            other_entities = total_entities - known_tags_sum\n",
    "            frequencies.append(other_entities)\n",
    "            entity_tags.append('Other')\n",
    "\n",
    "        if total_entities > 0:\n",
    "            percentages = [(freq / total_entities) * 100 for freq in frequencies]  # Calculate percentages\n",
    "        else:\n",
    "            percentages = [0] * len(frequencies)\n",
    "\n",
    "        colors = [entity_colors[tag] for tag in entity_tags]\n",
    "\n",
    "        axes[index].bar(entity_tags, frequencies, color=colors)\n",
    "        axes[index].set_title(company)\n",
    "        axes[index].set_ylim(0, max_frequency)  # Use common y-axis height\n",
    "        \n",
    "        if show_percent:\n",
    "            for j, (freq, percent) in enumerate(zip(frequencies, percentages)):\n",
    "                axes[index].text(j, freq, f'{percent:.0f}%', ha='center', va='bottom', fontsize=10, color='black')\n",
    "        else:\n",
    "            for j, freq in enumerate(frequencies):\n",
    "                axes[index].text(j, freq , str(freq), ha='center', va='bottom', fontsize=10, color='black')\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/histograms_{\"with_percent\" if show_percent else \"with_numbers\"}_{\"with_Other\" if show_other else \"\"}_{depth}.png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node distribution histographs\n",
    "with open('data/triplets_no_cutoff/graphs.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "with open('results/basic_stats.json', 'r') as file:\n",
    "    statistics = json.load(file)\n",
    "\n",
    "depth = 'all'\n",
    "plot_selected_histograms(statistics, depth, show_percent=False, show_other=True)\n",
    "\n",
    "if False:\n",
    "    for depth in ['depth=1','depth=2','depth=3','all']:\n",
    "        for show_percent in [True, False]:\n",
    "            plot_selected_histograms(statistics, depth, show_percent=show_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of KGs for different depths\n",
    "with open('data/triplets_no_cutoff/graphs.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "def generate_kg_grid_for_depth(results, save_directory, direction, max_depth, skip_companies):\n",
    "    fig, axes = plt.subplots(5, 4, figsize=(12, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    index = 0\n",
    "    for root in results:\n",
    "        if root in skip_companies:\n",
    "            continue\n",
    "        if index >= 20:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            root_node = (root, 'ORG')\n",
    "            G = hf.make_graph(results[root])\n",
    "            G_pruned = hf.prune_graph_by_depth(G, root_node, max_depth, direction)\n",
    "            num_nodes = G_pruned.number_of_nodes()\n",
    "            \n",
    "            ax = axes[index]\n",
    "            pos = nx.spring_layout(G_pruned, k=2, iterations=100)\n",
    "\n",
    "            entity_colors = {\n",
    "                'LOC': '#ffb347',  # orange\n",
    "                'ORG': '#87ceeb',  # light blue\n",
    "                'PER': '#90ee90',  # light green\n",
    "                'MISC': '#dda0dd',  # plum\n",
    "            }\n",
    "\n",
    "            node_colors = []\n",
    "            node_sizes = []\n",
    "            labels = {}\n",
    "\n",
    "            for node in G_pruned.nodes():\n",
    "                entity_name, entity_type = node\n",
    "                labels[node] = entity_name\n",
    "\n",
    "                if node == root_node:\n",
    "                    node_colors.append('#d90000')  # Root node color (red)\n",
    "                    node_sizes.append(100)  # Root node size (also dynamic)\n",
    "                else:\n",
    "                    node_colors.append(entity_colors.get(entity_type, '#c08aed'))  # purple for any undefined types\n",
    "                    node_sizes.append(100)\n",
    "\n",
    "            nx.draw(G_pruned, pos, node_size=[s * 0.5 for s in node_sizes], node_color=node_colors,\n",
    "                    font_size=0, arrows=True, arrowsize=5, edge_color='gray', width=0.2, ax=ax)\n",
    "            \n",
    "            ax.set_title(f\"{root}\", fontsize=12)\n",
    "            index += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root} at depth {max_depth}: {e}\")\n",
    "            continue\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.suptitle(f\"Knowledge Graphs at Depth {max_depth} ({direction})\", fontsize=20, y=1.02)\n",
    "    \n",
    "    if save_directory:\n",
    "        filepath = os.path.join(save_directory, f\"kg_grid_depth_{max_depth}_{direction}.png\")\n",
    "        plt.savefig(filepath, format='png', bbox_inches='tight')\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "save_directory = 'results'\n",
    "direction = 'bidirectional_and'\n",
    "max_depths = [-1]  # Choose the depth level you want to visualize\n",
    "skip_companies = ['Elysium', 'HolosGen', 'Hyperion Power', 'StarCore Nuclear', 'Terrestial']\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    generate_kg_grid_for_depth(results, save_directory, direction, max_depth, skip_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get advanced KG statistics\n",
    "filepath = 'data/triplets_no_cutoff/graphs.json'\n",
    "\n",
    "with open(filepath, 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "for root in results:\n",
    "    root_node = (root, 'ORG')\n",
    "    result = results[root]\n",
    "    \n",
    "    # Skip empty graphs\n",
    "    if not result:\n",
    "        continue\n",
    "\n",
    "    # Otherwise, continue on\n",
    "    G = hf.make_graph(result)\n",
    "        \n",
    "    total_degree = G.degree(root_node)\n",
    "    in_degree = G.in_degree(root_node)\n",
    "    out_degree = G.out_degree(root_node)\n",
    "    in_div_out = in_degree/out_degree if out_degree > 0 else 0\n",
    "    \n",
    "    graph_stats = {\n",
    "        'total_degree': total_degree,\n",
    "        'in_degree': in_degree,\n",
    "        'out_degree': out_degree,\n",
    "        'in/out': in_div_out\n",
    "    }\n",
    "    \n",
    "    statistics[root] = graph_stats\n",
    "\n",
    "with open('results/degree_stats.json', 'w') as outfile:\n",
    "    json.dump(statistics, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
