{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, combine all entities and triplets\n",
    "all_data = []\n",
    "\n",
    "for filename in sorted(glob.glob('data/batches/entities_and_triplets_*.json')):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        all_data.extend(data)\n",
    "\n",
    "with open('data/raw_triplets.json', 'w') as output_file:\n",
    "    json.dump(all_data, output_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich triplets with entity type data\n",
    "with open('data/raw_triplets.json', 'r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "threshold = 0.8\n",
    "enriched_data = []\n",
    "enriched_triplets_all = []\n",
    "\n",
    "for instance in raw_data:\n",
    "\n",
    "    triplets = instance['triplets']\n",
    "    entities = instance['entities']\n",
    "    url = instance['url']\n",
    "\n",
    "    enriched_triplets = []\n",
    "\n",
    "    for triplet in triplets:\n",
    "\n",
    "        head = triplet['head']\n",
    "        tail = triplet['tail']\n",
    "\n",
    "        best_head = None\n",
    "        best_head_sim = threshold\n",
    "\n",
    "        best_tail = None\n",
    "        best_tail_sim = threshold\n",
    "\n",
    "        for entity in entities:\n",
    "            name = entity['word']\n",
    "            head_sim = Levenshtein.ratio(name, head)\n",
    "            tail_sim = Levenshtein.ratio(name, tail)\n",
    "\n",
    "            if head_sim > best_head_sim:\n",
    "                best_head_sim = head_sim\n",
    "                best_head = {\n",
    "                    'word': name,\n",
    "                    'entity': entity['entity']\n",
    "                }\n",
    "            \n",
    "            if tail_sim > best_tail_sim:\n",
    "                best_tail_sim = tail_sim\n",
    "                best_tail = {\n",
    "                    'word': name,\n",
    "                    'entity': entity['entity']\n",
    "                }\n",
    "    \n",
    "    \n",
    "        enriched_triplets.append({\n",
    "            'head': best_head if best_head else {'word': head, 'entity': 'Unknown'},\n",
    "            'relation': triplet['relation'],\n",
    "            'tail': best_tail if best_tail else {'word': tail, 'entity': 'Unknown'},\n",
    "        })\n",
    "    \n",
    "    enriched_data.append({\n",
    "        'idx': instance['idx'],\n",
    "        'url': url,\n",
    "        'enriched_triplets': enriched_triplets,\n",
    "        'original_triplets': triplets,\n",
    "        'all_entities': entities\n",
    "    })\n",
    "    enriched_triplets_all.extend(enriched_triplets)\n",
    "\n",
    "with open('data/enriched_triplets.json', 'w') as file:\n",
    "    json.dump(enriched_data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_replacements = {\n",
    "    # Demonyms\n",
    "    'Russian': 'Russia', 'Chinese': 'Chinese', \n",
    "    'American': 'United States', 'British': 'United Kingdom', 'Polish': 'Poland',\n",
    "    'Swedish': 'Sweden', 'French': 'France', 'South Korean': 'South Korea', \n",
    "    'Dutch': 'Netherlands', 'Ukranian': 'Ukraine',\n",
    "\n",
    "    # Countries etc.\n",
    "    'UK': 'United Kingdom', 'US': 'United States', 'UAE': 'United Arab Emirates',\n",
    "    'Russian Federation': 'Russia', 'Czech': 'Czech Republic', 'Netherlands': 'Netherlands',\n",
    "    'Korea': 'South Korea',\n",
    "    'EU': 'European Union', 'UN': 'United Nations', \n",
    "\n",
    "    # Abbreviations\n",
    "    'WEC': 'Westinghouse', 'USNC': 'Ultra Safe Nuclear Corporation', 'GEH': 'GE Hitachi', 'GA': 'General Atomics',\n",
    "    'GFP': 'Global First Power', 'ORNL': 'Oak Ridge National Laboratory',\n",
    "    'IAEA': 'International Atomic Energy Agency'\n",
    "}\n",
    "\n",
    "org_replacements = { # Replace if entity type == ORG (substring match)\n",
    "    'ARC': 'ARC',\n",
    "    'Babcock': 'Babcock and Wilcox',\n",
    "    'BWX': 'BWX',\n",
    "    'Elysium': 'Elysium',\n",
    "    'Flibe': 'Flibe',\n",
    "    'Framatome': 'Framatome',\n",
    "    'Hitachi': 'GE Hitachi',\n",
    "    'General Atomics': 'General Atomics',\n",
    "    'Holos': 'HolosGen',\n",
    "    'Holtec': 'Holtec International',\n",
    "    'Hyperion': 'Hyperion Power',\n",
    "    'Kairos': 'Kairos Power',\n",
    "    'Moltex': 'Moltex Energy',\n",
    "    'NANO': 'NANO Nuclear',\n",
    "    'NuScale': 'NuScale',\n",
    "    'Oak Ridge': 'Oak Ridge National Laboratory',\n",
    "    'Oklo': 'Oklo',\n",
    "    'StarCore': 'StarCore Nuclear',\n",
    "    'TerraPower': 'TerraPower',\n",
    "    'Terrestial': 'Terrestial',\n",
    "    'ThorCon': 'ThorCon',\n",
    "    'Ultra Safe Nuclear': 'Ultra Safe Nuclear Corporation',\n",
    "    'Berkeley': 'Berkeley',\n",
    "    'Westinghouse': 'Westinghouse',\n",
    "    'X-Energy': 'X-Energy',\n",
    "\n",
    "    'Point Lepreau': 'Point Lepreau NPP',\n",
    "    'China General Nuclear ': 'China General Nuclear Power Corporation',\n",
    "    'Clinch River': 'Clinch River Site',\n",
    "    'East Tennessee Technology': 'East Tennessee Technology Park',\n",
    "    'Fukushima': 'Fukushima',\n",
    "    'Jacobs UK': 'Jacobs',\n",
    "    'Korea Electric Power': 'Korea Electric Power Company',\n",
    "    'Magnox': 'Magnox',\n",
    "    'ORLEN Synthos': 'ORLEN Synthos Green Energy',\n",
    "    'Rolls Royce': 'Rolls-Royce',\n",
    "    'Sizewell': 'Sizewell C',\n",
    "    'WEC': 'WEC Group',\n",
    "    'Temelin': 'Temelin NPP',\n",
    "    'TransAlta': 'TransAlta  Corporation'\n",
    "}\n",
    "\n",
    "per_replacements = { # Replace if entity type == PER (substring match)\n",
    "    'Grossi': 'Rafael Grossi'\n",
    "}\n",
    "\n",
    "loc_replacements = {\n",
    "    'Point Lepreau': 'Point Lepreau NPP'\n",
    "}\n",
    "\n",
    "letter_replacements = {\n",
    "    'à': 'a', 'À': 'A',\n",
    "    'á': 'a', 'Á': 'A',\n",
    "    'ä': 'a', 'Ä': 'A',\n",
    "    'ã': 'a', 'Ã': 'A',\n",
    "    'å': 'a', 'Å': 'A',\n",
    "    'ą': 'a', 'Ą': 'A',\n",
    "    'ç': 'c', 'Ç': 'C',\n",
    "    'č': 'c', 'Č': 'C',\n",
    "    'ď': 'd', 'Ď': 'D',\n",
    "    'é': 'e', 'É': 'E',\n",
    "    'è': 'e', 'È': 'E',\n",
    "    'ě': 'e', 'Ě': 'E',\n",
    "    'ğ': 'g', 'Ğ': 'G',\n",
    "    'í': 'i', 'Í': 'I',\n",
    "    'ï': 'i', 'Ï': 'I',\n",
    "    'ı': 'i', 'İ': 'I',\n",
    "    'ł': 'l', 'Ł': 'L',\n",
    "    'ł': 'l', 'Ł': 'L',\n",
    "    'ñ': 'n', 'Ñ': 'N',\n",
    "    'ň': 'n', 'Ň': 'N',\n",
    "    'ń': 'n', 'Ń': 'N',\n",
    "    'ó': 'o', 'Ó': 'O',\n",
    "    'ö': 'o', 'Ö': 'O',\n",
    "    'ô': 'o', 'Ô': 'O',\n",
    "    'õ': 'o', 'Õ': 'O',\n",
    "    'ø': 'o', 'Ø': 'O',\n",
    "    'ř': 'r', 'Ř': 'R',\n",
    "    'š': 's', 'Š': 'S',\n",
    "    'ť': 't', 'Ť': 'T',\n",
    "    'ú': 'u', 'Ú': 'U',\n",
    "    'ü': 'u', 'Ü': 'U',\n",
    "    'û': 'u', 'Û': 'U',\n",
    "    'ù': 'u', 'Ù': 'U',\n",
    "    'ý': 'y', 'Ý': 'Y',\n",
    "    'ž': 'z', 'Ž': 'Z',\n",
    "    '.': ' ', ' “': '',\n",
    "    '&': 'and',\n",
    "}\n",
    "\n",
    "def custom_replacement(word, entity=None):\n",
    "    new = None\n",
    "    \n",
    "    for letter, replacement in letter_replacements.items():\n",
    "        word = word.replace(letter, replacement)\n",
    "    \n",
    "    lower_word = word.strip().lower()\n",
    "\n",
    "    for key, replacement in exact_replacements.items():\n",
    "        if lower_word == key.lower():\n",
    "            new = replacement\n",
    "    \n",
    "    if entity == 'ORG':\n",
    "        for org_key, replacement in org_replacements.items():\n",
    "            if org_key.lower() in lower_word:\n",
    "                new = replacement\n",
    "    elif entity == 'PER':\n",
    "        for per_key, replacement in per_replacements.items():\n",
    "            if per_key.lower() in lower_word:\n",
    "                new = replacement\n",
    "    elif entity == 'LOC':\n",
    "        for loc_key, replacement in loc_replacements.items():\n",
    "            if loc_key.lower() in lower_word:\n",
    "                new = replacement\n",
    "    \n",
    "    #if not new:\n",
    "        #print(word, entity)\n",
    "    #if new and new != word:\n",
    "        #print(f\"Before: {word}\")\n",
    "        #print(f\"After: {new}\")\n",
    "    if not new:\n",
    "        new = word\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make into Excel for data prep\n",
    "filepath = 'data/unfiltered/all_triplets' # use the unfiltered data\n",
    "\n",
    "with open(filepath + '.json', 'r') as file:\n",
    "    triplets = json.load(file)\n",
    "\n",
    "only_triplets = []\n",
    "for triplet in triplets:\n",
    "\n",
    "    head = triplet['head']['word']\n",
    "    h_ent = triplet['head']['entity']\n",
    "    tail = triplet['tail']['word']\n",
    "    t_ent = triplet['tail']['entity']\n",
    "\n",
    "    # Drop triplets DistilBERT was not able to identify\n",
    "    if h_ent == 'Unknown' or t_ent == 'Unknown':\n",
    "        continue\n",
    "\n",
    "    head = custom_replacement(head, h_ent)\n",
    "    tail = custom_replacement(tail, t_ent)\n",
    "\n",
    "    only_triplets.append({\n",
    "        'head': head,\n",
    "        'h_ent': h_ent,\n",
    "        'relation': triplet['relation'],\n",
    "        'tail': tail,\n",
    "        't_ent': t_ent\n",
    "    })\n",
    "\n",
    "df = pd.json_normalize(only_triplets)\n",
    "df = df.drop_duplicates()\n",
    "df.to_excel('data/final_triplets.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
