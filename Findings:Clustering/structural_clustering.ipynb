{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the scenes: structural clustering of adjacency matrices\n",
    "### Please note that the adjacency matrices themselves are computed in Method:Knowledge_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "depth = 3 # Options: -1 (unpruned), 1, 2, 3\n",
    "with_tags = 'no' # Options: 'with' (yes) and 'no' (no)\n",
    "######\n",
    "\n",
    "filepath = f'../Method:Knowledge_graphs/data/triplets_no_cutoff/adj_matrices_reduced_rel_dim_pruned_{depth}_{with_tags}_tags.npz'\n",
    "loaded_data = np.load(filepath, allow_pickle=False)\n",
    "loaded_adj_matrices = {key: loaded_data[key] for key in loaded_data}\n",
    "print(f\"Finished loading adjacency matrices (depth={depth}, {with_tags} tags)\") \n",
    "print(f\"Shape: {loaded_adj_matrices['ARC'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform clustering and get Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract features\n",
    "def extract_features_from_adj_matrices(adj_matrices):\n",
    "    '''Flatten and concatenate the adjacency matrices'''\n",
    "    flattened_matrices = [adj_matrix.flatten() for adj_matrix in adj_matrices]\n",
    "    return np.concatenate(flattened_matrices)\n",
    "\n",
    "firm_features = []\n",
    "firm_names = []\n",
    "\n",
    "for firm, adj_matrices in loaded_adj_matrices.items():\n",
    "    if adj_matrices is not None:\n",
    "        features = extract_features_from_adj_matrices(adj_matrices)\n",
    "        firm_features.append(features)\n",
    "        firm_names.append(firm)\n",
    "\n",
    "firm_features = np.array(firm_features)\n",
    "\n",
    "# 2. Remove features with no explanatory power (i.e. var = 0)\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "firm_features_reduced = selector.fit_transform(firm_features)\n",
    "\n",
    "print(f\"Original number of features: {firm_features.shape[1]}\")\n",
    "print(f\"Number of features after removing zero variance: {firm_features_reduced.shape[1]}\")\n",
    "\n",
    "# 3. Normalise features\n",
    "scaler = StandardScaler()\n",
    "firm_features_normalized = scaler.fit_transform(firm_features_reduced)\n",
    "#np.save('data/firm_features.npy', firm_features_reduced)\n",
    "np.save('data/firm_features_normalized.npy', firm_features_normalized)\n",
    "np.save('data/firm_names.npy', firm_names)\n",
    "\n",
    "# 4. Compute Silhouette scores\n",
    "print(\"\\nSilhouette Scores:\")\n",
    "firm_features_normalized = np.load('data/firm_features_normalized.npy')\n",
    "max_K = 0\n",
    "max_score = 0\n",
    "for num_clusters in range(2,16):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(firm_features_normalized)\n",
    "    silhouette_avg = silhouette_score(firm_features_normalized, kmeans_labels)\n",
    "    if silhouette_avg > max_score:\n",
    "        max_score = silhouette_avg\n",
    "        max_K = num_clusters\n",
    "    print(f\"- K = {num_clusters}, Score = {silhouette_avg:.6f}\")\n",
    "\n",
    "print(f\"\\nRecommended number of clusters: {max_K} (Score: {max_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Clusters\n",
    "##### The hyperparam n_clusters determines how many clusters (recommended K given above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kmeans_plot(n_clusters=2, depth=-1, with_tags='no', exclude_firms=None):\n",
    "    if n_clusters < 2:\n",
    "        raise KeyError(\"Please choose a K higher than 1\")\n",
    "\n",
    "    # Load data\n",
    "    firm_features_normalized = np.load('data/firm_features_normalized.npy')\n",
    "    firm_names = np.load('data/firm_names.npy')\n",
    "\n",
    "    # Exclude specified firms\n",
    "    if exclude_firms is not None:\n",
    "        exclude_indices = [i for i, firm in enumerate(firm_names) if firm in exclude_firms]\n",
    "        firm_features_normalized = np.delete(firm_features_normalized, exclude_indices, axis=0)\n",
    "        firm_names = np.delete(firm_names, exclude_indices, axis=0)\n",
    "\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(firm_features_normalized)\n",
    "    sil_score = silhouette_score(firm_features_normalized, kmeans_labels)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = IncrementalPCA(n_components=2, batch_size=200)\n",
    "    reduced_features = pca.fit_transform(firm_features_normalized)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=kmeans_labels, cmap='viridis')\n",
    "\n",
    "    texts = []\n",
    "    for i, (firm, cluster) in enumerate(zip(firm_names, kmeans_labels)):\n",
    "        texts.append(plt.text(reduced_features[i, 0], reduced_features[i, 1], firm, fontsize=10, alpha=0.75))\n",
    "\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.8))\n",
    "\n",
    "    # Add info\n",
    "    prune_text = 'no'\n",
    "    if depth > 0:\n",
    "        prune_text = depth\n",
    "    textstr = f\"{with_tags.capitalize()} tags\\nPruned: {prune_text}\\n\\nNum clusters: {n_clusters}\\nSil score: {sil_score:.3f}\"\n",
    "    props = dict(boxstyle='square,pad=0.5', facecolor='lightgrey', edgecolor='black', alpha=0.75)\n",
    "    plt.text(0.04, 0.80, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "             verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
    "\n",
    "    # NOTE: the two first parameters in plt.text() determines vertical and horizontal position of grey box\n",
    "    # Change these params to re-position grey box if it overlaps with data points\n",
    "    # Recommended: \n",
    "    #   (0.04, 0.80) for top left\n",
    "    #   (0.85, 0.80) for top right\n",
    "    #   (0.85, 0.05) for bottom right\n",
    "\n",
    "    filepath = f\"results/kmeans_structural/kmeans_pruned_{depth}_{with_tags}_tags_{n_clusters}_clusters\"\n",
    "    if exclude_firms:\n",
    "        filepath += f'_excluded{len(exclude_firms)}'\n",
    "    filepath += f\"_{str(int(round(sil_score, 2)*100))}\"\n",
    "    print(f\"Saving plot to {filepath}\")\n",
    "\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_firms = None\n",
    "#exclude_firms = ['Ultra Safe Nuclear Corporation', 'Babcock and Wilcox', 'General Atomics']\n",
    "#exclude_firms = ['Westinghouse', 'Framatome']\n",
    "#exclude_firms = ['Westinghouse']\n",
    "#exclude_firms = ['NuScale', 'Westinghouse']\n",
    "#exclude_firms = ['Westinghouse', 'NuScale', 'GE Hitachi']\n",
    "\n",
    "for K in [3]:\n",
    "    make_kmeans_plot(n_clusters=K, depth=depth, with_tags=with_tags, exclude_firms=exclude_firms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Hierarchical KMeans clustering\n",
    "firm_features_normalized = np.load('data/firm_features_normalized.npy')\n",
    "firm_names = np.load('data/firm_names.npy')\n",
    "\n",
    "# First level\n",
    "kmeans_level_1 = KMeans(n_clusters=2, random_state=42)\n",
    "first_level_labels = kmeans_level_1.fit_predict(firm_features_normalized)\n",
    "\n",
    "second_level_labels = np.zeros_like(first_level_labels)\n",
    "\n",
    "# Second-level clustering\n",
    "for cluster in np.unique(first_level_labels):\n",
    "    cluster_indices = np.where(first_level_labels == cluster)[0]\n",
    "\n",
    "    if len(cluster_indices) < 3:\n",
    "        print(f\"Cluster {cluster} has too few points for further splitting.\")\n",
    "        continue\n",
    "\n",
    "    best_score = -1\n",
    "    best_n_clusters = 2\n",
    "\n",
    "    print(f'Testing for Cluster {cluster}:')\n",
    "\n",
    "    for n_clusters in range(2,7):\n",
    "        try:\n",
    "            kmeans_tmp = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            sub_cluster_labels = kmeans_tmp.fit_predict(firm_features_normalized[cluster_indices])\n",
    "            score = silhouette_score(firm_features_normalized[cluster_indices], sub_cluster_labels)\n",
    "            print(f'Num Clusters: {n_clusters}, Silhouette Score: {score:.3f}')\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_n_clusters = n_clusters\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    kmeans_level_2 = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "    sub_cluster_labels = kmeans_level_2.fit_predict(firm_features_normalized[cluster_indices])\n",
    "    second_level_labels[cluster_indices] = sub_cluster_labels + (cluster * 10)\n",
    "\n",
    "pca = IncrementalPCA(n_components=2, batch_size=200)\n",
    "reduced_features = pca.fit_transform(firm_features_normalized)\n",
    "markers = cycle(['o', 's', 'v', '^', '<', '>', 'P', '*', 'X', 'D'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(second_level_labels):\n",
    "    marker = next(markers)\n",
    "    plt.scatter(reduced_features[second_level_labels == cluster, 0],\n",
    "                reduced_features[second_level_labels == cluster, 1],\n",
    "                label=f'Sub-Cluster {cluster}',\n",
    "                marker=marker)\n",
    "\n",
    "texts = []\n",
    "for i, (firm, cluster) in enumerate(zip(firm_names, kmeans_labels)):\n",
    "    texts.append(plt.text(reduced_features[i, 0], reduced_features[i, 1], firm, fontsize=8, alpha=0.75))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
    "\n",
    "#scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=second_level_labels, cmap='tab20')\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Sub-Clusters\")\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Nested KMeans Clustering Visualization with PCA')\n",
    "plt.legend()\n",
    "plt.savefig(f'results/new_kmeans_hierarchical_2_N_clusters')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
